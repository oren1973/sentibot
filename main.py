# main.py
import os
import sys
import pandas as pd
from datetime import datetime, date
import logging
import json # 住驻转 注专 砖专转 raw_scores_details -JSON

from settings import (
    setup_logger, NEWS_SOURCES_CONFIG, MAIN_MAX_TOTAL_HEADLINES,
    REDDIT_ENABLED, REDDIT_SUBREDDITS, REDDIT_LIMIT_PER_SUBREDDIT, REDDIT_COMMENTS_PER_POST,
    REPORTS_OUTPUT_DIR, LEARNING_LOG_CSV_PATH
)
from smart_universe import SYMBOLS 
from news_aggregator import fetch_all_news 
from reddit_scraper import get_reddit_posts
from sentiment_analyzer import analyze_sentiment
from recommender import make_recommendation
from alpaca_trader import trade_stock
from email_sender import send_run_success_email

logger = setup_logger("SentibotMain")

def load_learning_log() -> pd.DataFrame:
    # 专 DataFrame 专拽 拽专 砖爪专 专 转
    empty_log_df = pd.DataFrame(columns=[
        "run_id", "symbol", "datetime", "sentiment_avg", "sentiment_std", 
        "num_total_articles", "main_source_overall",
        "decision", "previous_decision", "trade_executed", "raw_scores_details"
    ])

    logger.info(f"Checking for existing learning log at: {LEARNING_LOG_CSV_PATH}")
    if not os.path.exists(LEARNING_LOG_CSV_PATH):
        logger.warning(f"Learning log file NOT FOUND at {LEARNING_LOG_CSV_PATH}. Will start with an empty log.")
        return empty_log_df

    try:
        logger.info(f"Attempting to read CSV: {LEARNING_LOG_CSV_PATH}")
        df = pd.read_csv(LEARNING_LOG_CSV_PATH)
        logger.info(f"Successfully read {len(df)} rows from {LEARNING_LOG_CSV_PATH}.")
        
        if df.empty:
            logger.info(f"Learning log file {LEARNING_LOG_CSV_PATH} was read but is empty. Starting with an empty log.")
            return empty_log_df
            
        if 'datetime' not in df.columns:
            logger.error(f"'datetime' column MISSING in {LEARNING_LOG_CSV_PATH}. Cannot process previous decisions. Returning empty log.")
            return empty_log_df

        # 住 专 转 注转 转专
        # 砖专 注转拽 砖 注 拽专转 拽专 砖 注转 
        df['datetime_original_str'] = df['datetime'].astype(str) 
        
        # 住 驻专 ISO8601 转,  转 -Pandas 住转 住拽   砖
        try:
            # 砖: pandas to_datetime 注 format='ISO8601' 爪驻 -Z 住祝  offset.
            #  -ISO 砖  naive ( 注 timezone), 注祝 infer_datetime_format   爪 format.
            # 砖转砖 - infer_datetime_format  转 砖 转专 驻专 拽 砖 -ISO string.
            df['datetime_parsed'] = pd.to_datetime(df['datetime_original_str'], errors='coerce', infer_datetime_format=True)
            #  转 注 砖 专 转 ISO8601 转拽, 驻砖专 :
            # df['datetime_parsed'] = pd.to_datetime(df['datetime_original_str'], format='iso8601', errors='coerce')
            logger.info("Attempted to parse 'datetime' column (inferring format or using ISO8601).")
        except Exception as e_parse_dt: # 转驻住转 砖 转 转专  to_datetime 注爪 砖 驻  爪驻
            logger.error(f"Unexpected error during pd.to_datetime conversion: {e_parse_dt}", exc_info=True)
            df['datetime_parsed'] = pd.NaT # 住 砖

        num_failed_parsing = df['datetime_parsed'].isnull().sum()
        if num_failed_parsing > 0:
            logger.warning(f"Could not parse {num_failed_parsing} datetime strings in 'datetime' column.")
            failed_examples = df[df['datetime_parsed'].isnull()]['datetime_original_str'].head().tolist()
            logger.warning(f"Examples of original datetime strings that failed parsing: {failed_examples}")
        
        original_len = len(df)
        df.dropna(subset=['datetime_parsed'], inplace=True) # 住专 砖专转 注 NaT
        df['datetime'] = df['datetime_parsed'] 
        df.drop(columns=['datetime_parsed', 'datetime_original_str'], inplace=True, errors='ignore')
        
        if len(df) < original_len:
            logger.warning(f"Removed {original_len - len(df)} rows with invalid datetime format from learning log after parsing attempt.")
        
        if df.empty:
            logger.warning(f"Learning log became empty after datetime parsing and cleanup. Starting with an empty log.")
            return empty_log_df
        
        logger.info(f"Successfully loaded and processed learning log with {len(df)} valid entries after datetime conversion.")
        return df
    
    except pd.errors.EmptyDataError:
        logger.warning(f"Pandas EmptyDataError: The file {LEARNING_LOG_CSV_PATH} is empty or unreadable as CSV. Starting with an empty log.")
        return empty_log_df
    except Exception as e:
        logger.error(f"CRITICAL error loading or processing learning log from {LEARNING_LOG_CSV_PATH}: {e}. Starting with an empty log.", exc_info=True)
        return empty_log_df

def save_learning_log_entry(log_df: pd.DataFrame, new_entry_data: dict):
    try:
        new_entry_df = pd.DataFrame([new_entry_data])
        if log_df.empty:
            log_df = new_entry_df
        else:
            log_df = pd.concat([log_df, new_entry_df], ignore_index=True)
        
        #  砖注转 datetime  住 datetime 砖 pandas 驻 砖专
        if 'datetime' in log_df.columns:
            log_df['datetime'] = pd.to_datetime(log_df['datetime'])
            # 砖专 驻专 ISO8601 住专 注拽
            # Pandas 砖专 转   转  注  住 datetime.
            #  专爪 砖  注 驻专 专转 -CSV:
            # df_to_save = log_df.copy()
            # df_to_save['datetime'] = df_to_save['datetime'].dt.isoformat()
            # df_to_save.to_csv(LEARNING_LOG_CSV_PATH, index=False, encoding='utf-8-sig')
        
        log_df.to_csv(LEARNING_LOG_CSV_PATH, index=False, encoding='utf-8-sig')
        logger.info(f"Saved new entry to learning log. Total entries: {len(log_df)}")
        return log_df
    except Exception as e:
        logger.error(f"Error saving entry to learning log {LEARNING_LOG_CSV_PATH}: {e}", exc_info=True)
        return log_df

def main(force_run: bool = False):
    run_id_str = datetime.now().strftime("%Y%m%d_%H%M%S")
    # 砖专 转专 砖注 驻专 ISO8601,  注专 注拽转 拽专 砖专
    current_datetime_iso = datetime.now().isoformat(timespec='microseconds') 
    logger.info(f" Starting Sentibot run ID: {run_id_str}")
    
    # --- 拽注 拽  砖 砖 拽抓  ( 爪专) ---
    # old_log_rename = os.getenv("OLD_LOG_PATH_TO_RENAME")
    # new_log_rename = os.getenv("NEW_LOG_PATH_AFTER_RENAME")
    # if old_log_rename and new_log_rename:
    #     logger.info(f"Attempting to rename log file from '{old_log_rename}' to '{new_log_rename}' based on environment variables.")
    #     try:
    #         if os.path.exists(old_log_rename):
    #             if os.path.exists(new_log_rename):
    #                 logger.warning(f"Target log file '{new_log_rename}' already exists. Deleting it before rename.")
    #                 os.remove(new_log_rename)
    #             os.rename(old_log_rename, new_log_rename)
    #             logger.info(f"Successfully renamed '{old_log_rename}' to '{new_log_rename}'.")
    #             logger.warning("Please remove OLD_LOG_PATH_TO_RENAME and NEW_LOG_PATH_AFTER_RENAME environment variables after this run.")
    #         else:
    #             logger.warning(f"Old log file '{old_log_rename}' not found. Cannot rename.")
    #     except Exception as e_rename:
    #         logger.error(f"Failed to rename log file: {e_rename}", exc_info=True)
    # --- 住祝 拽注 拽  ---

    learning_log_df = load_learning_log()
    
    try:
        os.makedirs(REPORTS_OUTPUT_DIR, exist_ok=True)
        logger.info(f"Reports will be saved to directory: {REPORTS_OUTPUT_DIR}")
    except OSError as e_dir:
        logger.error(f"Could not create reports directory '{REPORTS_OUTPUT_DIR}': {e_dir}. Using current directory for reports.")
        #  爪专转 转拽 砖, 砖 转 转 专专转  转拽 转
        # global REPORTS_OUTPUT_DIR #  REPORTS_OUTPUT_DIR   转 砖
        # REPORTS_OUTPUT_DIR = "." #  驻 专 转 爪专

    all_individual_headline_analysis = []
    aggregated_symbol_analysis = []

    if not SYMBOLS or not isinstance(SYMBOLS, list) or len(SYMBOLS) == 0:
        logger.critical("SYMBOLS list is not defined or empty in smart_universe.py. Exiting application.")
        return

    logger.info(f"Processing symbols: {', '.join(SYMBOLS)}")

    for symbol in SYMBOLS:
        logger.info(f"--- Processing symbol: {symbol} ---")
        symbol_headlines_data = [] # 专砖 砖 tuples: (text, source_name_from_scraper)
        try:
            # 住祝 砖转 拽专转 砖专 -NEWS_SOURCES_CONFIG
            news_headlines_from_aggregator = fetch_all_news(symbol, max_headlines_total=MAIN_MAX_TOTAL_HEADLINES)
            if news_headlines_from_aggregator:
                logger.info(f"Fetched {len(news_headlines_from_aggregator)} headlines from news aggregator for '{symbol}'.")
                # 住祝 转 砖 拽专 驻 砖 专 -NEWS_SOURCES_CONFIG (驻转 )
                # fetch_all_news 专 专 专 tuple 注 砖 拽专
                symbol_headlines_data.extend(news_headlines_from_aggregator)
            else:
                logger.info(f"No headlines from news aggregator for '{symbol}'.")

            # 住祝 转 专  驻砖专
            if REDDIT_ENABLED:
                logger.info(f"Fetching Reddit content for '{symbol}'...")
                reddit_content = get_reddit_posts(
                    symbol=symbol,
                    subreddits_list=REDDIT_SUBREDDITS,
                    limit_per_sub=REDDIT_LIMIT_PER_SUBREDDIT,
                    comments_limit=REDDIT_COMMENTS_PER_POST
                ) # get_reddit_posts 专 专 list[tuple[str, str]] 砖专 -str 砖  "Reddit_Post"  "Reddit_Comment"
                if reddit_content:
                    logger.info(f"Fetched {len(reddit_content)} items (posts/comments) from Reddit for '{symbol}'.")
                    symbol_headlines_data.extend(reddit_content)
                else:
                    logger.info(f"No content from Reddit for '{symbol}'.")
            else:
                logger.info("Reddit fetching is disabled in settings.")
            
            if not symbol_headlines_data:
                logger.warning(f"No headlines or content found for '{symbol}' from any source after aggregation. Skipping symbol.")
                continue

            #  转 住驻专 驻专  转   专 -MAIN_MAX_TOTAL_HEADLINES
            # (专转 砖-fetch_all_news  专    驻转)
            if len(symbol_headlines_data) > MAIN_MAX_TOTAL_HEADLINES:
                logger.info(f"Capping total items for '{symbol}' from {len(symbol_headlines_data)} to {MAIN_MAX_TOTAL_HEADLINES}.")
                symbol_headlines_data = symbol_headlines_data[:MAIN_MAX_TOTAL_HEADLINES]

            logger.info(f"Total {len(symbol_headlines_data)} items (headlines/posts) for '{symbol}' to analyze.")
            
            current_symbol_sentiments_details = [] #  {'score': float, 'source': str}

            for item_text, source_key_from_scraper in symbol_headlines_data:
                # -source_key_from_scraper 爪专 转 驻转 砖转 -NEWS_SOURCES_CONFIG  "Reddit_Post"/"Reddit_Comment"
                logger.debug(f"  Analyzing: [{source_key_from_scraper}] '{item_text[:80]}...'")
                try:
                    # sentiment_analyzer 砖转砖 -source_key_from_scraper  爪 转 砖拽 
                    sentiment_score = analyze_sentiment(text=item_text, source_name=source_key_from_scraper) 
                    if sentiment_score is not None:
                        all_individual_headline_analysis.append({
                            "run_id": run_id_str, "symbol": symbol, "source": source_key_from_scraper,
                            "title_or_text": item_text, "sentiment_score": sentiment_score,
                            "analysis_timestamp": current_datetime_iso
                        })
                        current_symbol_sentiments_details.append({'score': sentiment_score, 'source': source_key_from_scraper})
                    else:
                        logger.warning(f"Sentiment analysis returned None for item from '{source_key_from_scraper}' for '{symbol}'.")
                except Exception as e_sentiment:
                    logger.error(f"Error during sentiment analysis for item from '{source_key_from_scraper}' for '{symbol}': '{item_text[:50]}...'. Error: {e_sentiment}", exc_info=False)

            if not current_symbol_sentiments_details:
                logger.warning(f"No sentiment scores were successfully calculated for '{symbol}'. Skipping recommendation for this symbol.")
                continue
            
            sentiment_scores_list = [s['score'] for s in current_symbol_sentiments_details]
            # 砖 砖 avg_sentiment_for_symbol 爪专 转爪注 -recommender  拽转 砖 砖拽   专 -analyze_sentiment
            # 专注,  砖-analyze_sentiment 专 爪 住驻 ( 专 砖拽)
            avg_sentiment_for_symbol = sum(sentiment_scores_list) / len(sentiment_scores_list) if sentiment_scores_list else 0.0
            sentiment_std_for_symbol = pd.Series(sentiment_scores_list).std() if len(sentiment_scores_list) > 1 else 0.0
            
            # 拽注转 拽专  注 住 住驻专 驻专  拽专
            source_names_for_symbol = [s['source'] for s in current_symbol_sentiments_details]
            source_counts = pd.Series(source_names_for_symbol).value_counts()
            main_source_overall_str = source_counts.index[0] if not source_counts.empty else "N/A"

            logger.info(f"Average sentiment for '{symbol}': {avg_sentiment_for_symbol:.4f} (Std: {sentiment_std_for_symbol:.4f}, Based on {len(sentiment_scores_list)} items, Main source by count: {main_source_overall_str})")

            # recommender.py 拽 转 爪注   驻 住驻 砖专 -settings
            recommendation_output = make_recommendation(avg_sentiment_for_symbol)
            current_trade_decision = recommendation_output.get("decision", "ERROR_NO_DECISION").upper() #  砖 转 转转 转
            
            logger.info(f"Recommendation for '{symbol}': {current_trade_decision} (Based on raw average score: {avg_sentiment_for_symbol:.4f})")
            
            previous_decision_for_symbol = "N/A"
            if not learning_log_df.empty and symbol in learning_log_df['symbol'].values:
                symbol_specific_log = learning_log_df[learning_log_df['symbol'] == symbol].sort_values(by='datetime', ascending=False)
                if not symbol_specific_log.empty:
                    previous_decision_for_symbol = str(symbol_specific_log.iloc[0]['decision']).upper() #  砖  转转 转 砖
            
            logger.info(f"Previous decision for '{symbol}' from cumulative log: {previous_decision_for_symbol}, Current decision: {current_trade_decision}")

            trade_action_taken = False
            if current_trade_decision == "BUY" and current_trade_decision != previous_decision_for_symbol:
                logger.info(f"Decision changed for {symbol} from '{previous_decision_for_symbol}' to BUY. Attempting trade.")
                trade_action_taken = trade_stock(symbol=symbol, decision="buy")
            elif current_trade_decision == "SELL" and previous_decision_for_symbol == "BUY": 
                logger.info(f"Decision changed for {symbol} from BUY to SELL. Attempting to close long position.")
                trade_action_taken = trade_stock(symbol=symbol, decision="sell")
            elif current_trade_decision == "SELL" and previous_decision_for_symbol != "BUY":
                logger.info(f"Decision is SELL for {symbol}, but no prior BUY position or prior decision was not BUY. Current policy is not to short sell. No trade action taken.")
            # 砖拽 拽专 砖  拽转 转 SELL (专 住专) 转  BUY (住)
            # elif current_trade_decision == "BUY" and previous_decision_for_symbol == "SELL":
            #     logger.info(f"Decision changed for {symbol} from SELL to BUY. Attempting to cover short position.")
            #     trade_action_taken = trade_stock(symbol=symbol, decision="buy") # 驻拽转 拽 住
            else: 
                logger.info(f"No trade action needed for {symbol}. Decision: {current_trade_decision}, Previous: {previous_decision_for_symbol}")

            learning_log_entry = {
                "run_id": run_id_str, "symbol": symbol, "datetime": current_datetime_iso,
                "sentiment_avg": round(avg_sentiment_for_symbol, 4),
                "sentiment_std": round(sentiment_std_for_symbol, 4),
                "num_total_articles": len(sentiment_scores_list),
                "main_source_overall": main_source_overall_str,
                "decision": current_trade_decision,
                "previous_decision": previous_decision_for_symbol,
                "trade_executed": trade_action_taken,
                "raw_scores_details": json.dumps(current_symbol_sentiments_details) # 砖专 -JSON string
            }
            learning_log_df = save_learning_log_entry(learning_log_df, learning_log_entry)
            
            aggregated_symbol_analysis.append({
                "run_id": run_id_str, "symbol": symbol, "avg_sentiment_score": round(avg_sentiment_for_symbol, 4),
                "num_analyzed_headlines": len(sentiment_scores_list), "trade_decision": current_trade_decision,
                "previous_decision_logged": previous_decision_for_symbol, 
                "trade_attempted": trade_action_taken, # 砖 注 砖  拽 砖,  转拽
                "processing_datetime": current_datetime_iso
            })

        except Exception as e_symbol_processing:
            logger.error(f"A critical error occurred while processing symbol '{symbol}': {e_symbol_processing}", exc_info=True)

    # --- 砖专转 转  ---
    daily_summary_report_filepath = None
    daily_detailed_report_filepath = None #  砖砖 专注 砖 

    if all_individual_headline_analysis: #  驻专
        detailed_report_df = pd.DataFrame(all_individual_headline_analysis)
        detailed_report_filename = f"detailed_headlines_{run_id_str}.csv"
        daily_detailed_report_filepath = os.path.join(REPORTS_OUTPUT_DIR, detailed_report_filename)
        try:
            detailed_report_df.to_csv(daily_detailed_report_filepath, index=False, encoding='utf-8-sig')
            logger.info(f" Saved daily detailed headline report: {daily_detailed_report_filepath}")
        except Exception as e_save_detail:
            logger.error(f"Failed to save daily detailed report to '{daily_detailed_report_filepath}': {e_save_detail}")
            daily_detailed_report_filepath = None 

    if aggregated_symbol_analysis: #  住 
        summary_report_df = pd.DataFrame(aggregated_symbol_analysis)
        # 住专 转  住 驻 住  转专爪
        summary_report_df = summary_report_df.sort_values(by="avg_sentiment_score", ascending=False)
        summary_report_filename = f"summary_decisions_{run_id_str}.csv"
        daily_summary_report_filepath = os.path.join(REPORTS_OUTPUT_DIR, summary_report_filename)
        try:
            summary_report_df.to_csv(daily_summary_report_filepath, index=False, encoding='utf-8-sig')
            logger.info(f" Saved daily aggregated symbol analysis report: {daily_summary_report_filepath}")
        except Exception as e_save_summary:
            logger.error(f"Failed to save daily summary report to '{daily_summary_report_filepath}': {e_save_summary}")
            daily_summary_report_filepath = None 
    
    # --- 砖转  注 转 ---
    attachments_to_send = []
    if daily_summary_report_filepath and os.path.exists(daily_summary_report_filepath):
        attachments_to_send.append(daily_summary_report_filepath)
        logger.info(f"Added daily summary report to email attachments: {daily_summary_report_filepath}")
    
    if os.path.exists(LEARNING_LOG_CSV_PATH):
        attachments_to_send.append(LEARNING_LOG_CSV_PATH)
        logger.info(f"Added cumulative learning log to email attachments: {LEARNING_LOG_CSV_PATH}")
    else:
        logger.warning(f"Cumulative log file not found at {LEARNING_LOG_CSV_PATH}, will not be attached to email.")

    #  转专爪 爪专祝  转  驻专:
    # if daily_detailed_report_filepath and os.path.exists(daily_detailed_report_filepath):
    #     attachments_to_send.append(daily_detailed_report_filepath)
    #     logger.info(f"Added daily detailed report to email attachments: {daily_detailed_report_filepath}")

    if 'send_run_success_email' in globals() and callable(send_run_success_email):
        if attachments_to_send: 
            logger.info(f"Attempting to send summary email for run ID: {run_id_str} with attachments: {attachments_to_send}")
            email_sent_successfully = send_run_success_email(
                run_id_str=run_id_str, 
                attachment_paths=attachments_to_send 
            )
            if email_sent_successfully:
                logger.info(f" Summary email for run {run_id_str} sent/attempted successfully.")
            else:
                logger.error(f" Failed to send summary email for run {run_id_str}.")
        else:
            logger.info(f"No reports generated or cumulative log found to attach. Skipping email for run ID: {run_id_str}.")
    
    logger.info(f" Sentibot run ID {run_id_str} finished.")

if __name__ == "__main__":
    force_run_param = False
    # 拽  专 "force" 砖 砖专转 驻拽
    if len(sys.argv) > 1 and sys.argv[1].lower() == "force":
        logger.info("Force run argument detected via command line.") 
        force_run_param = True
    
    main(force_run=force_run_param)
